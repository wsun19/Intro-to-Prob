February 27 (Discrete Random Variables) through April 14 (Transformation Theorem)


Discrete random variables in general
Given pmf, how to compute probabilities
Named discrete distributions: (how these distributions arise)
    Bernoulli
    Binomial
    Geometric
    Negative Binomial
    Poisson
    Hypergeometric
CDFs
Expected value - definition/properties
Law of Unconscious Statistician
Computing Means of named distribution
Linearity of expectation
Variance - definition and properties
Variance of named distributions
* Tail probability approach to Expected Value (?)

Continuous random variables / PDFs / CDFs
    Expected value
    Variance
Named distributions
    Uniform
    Gamma
    Normal
    Exp
    Beta
    Chi-square
    Bivariate normal
    F-distribution
Law of Unconscious Statistician
Linear transformation of normal is normal
Normal random variables -> Standard normal -> Compute probability
Euler Gamma function -> factorial, reduction property \Gamma(a + 1) = a \Gamma(a) for a > 0
Gamma density (a, b) -> a = shape, b = scale / Chi-square(n) = Gamma(n/2, 2)

Working with Gamma/Normalization tricks
Beta(a, b) distribution
Computing mean, variance of named distributions
CDF method. Univariate transformations
    Special case of 1-1 (monotone) maps -> baby (?) transformation theorem
Weibull, Log normal distributions as functions of other named distributions

Joint distributions (Compute prob / continuous prob / marginal prob)
    Joint-discrete pmf, pdf
    Marginal pmfs, marginal pdfs
    Conditional pmfs, conditional pmfs

Independence
Sum of random variables, sums of independent random variables, convolution formula
    Binomial theorem
Ordered statistics (max, min) - distributions associated with univariate statistics

Method of Jacobians/transformation theorem
