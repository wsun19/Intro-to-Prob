---
title: Intro Prob Lecture Notes
author:
    - William Sun 
date:
    - April 17, 2017
geometry: margin=3cm
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---
# Expected Value (Continued)
- Now interested in extending this concept to functions of more than one variable. Suppose $X, Y$ are jointly distributed and $g: \mathbb{R}^2 \rightarrow \mathbb{R}$ is any function. Then $$\mathbb{E}(g(X, Y)) = \sum\limits_x \sum\limits_y g(x, y) P_{X, Y}(x, y)$$ if $X, Y$ are jointly discrete. (Similar to Law of Unconscious Statistician) Otherwise, $$\mathbb{E}(g(X, Y)) = \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} g(x, y) f_{X, Y}(x, y) dx dy$$ if jointly continuous. And, if one is discrete, and the other is continuous, then we'll have both a sum and integral.
- Example: Suppose $X, Y \sim f_{X, Y}(x, y) = x e^{-x(1 + y)}$ for $x > 0, y > 0$. Compute 
-
\begin{align*}
	\mathbb{E}\Big(\frac{X}{1 + Y}\Big) &= \int\limits_0^{\infty} \int\limits_0^{\infty} \frac{x}{1 + y} xe^{-x(1 + y)} dx dy \\
	&= \int\limits_0^{\infty} \frac{1}{1 + y} \Big( \int\limits_0^{\infty}  x^2e^{-x(1 + y)} dx \Big) dy \\
	& \text{Content in parentheses is equal to } \frac{1}{(1 + y)^3} \Gamma(3) \text{ because ... Gamma distribution}\\
	&= \int\limits_0^{\infty} \frac{1}{1 + y} \cdot \frac{2}{(1 + y)^3} dy \\
	&= \dots
\end{align*}
- Example: Suppose $X_1, X_2 \sim$ independent Uniform(0, 1)
	- Intuition: Arrival time of two people to lunch
	- Find the expected time that the first person to arrive has to wait for the second person to arrive 
	- $Y_1 = min \{X_1, X_2\}, Y_2 = max \{X_1, X_2 \}$
	-
	\begin{align*}
		\mathbb{E}(Y_2 - Y_1) &= \int \int (y_2 - y_1) f_{Y_1, Y_2} (y_1, y_2) d{y_1} d{y_2} \\
		& \dots \\
		P(Y_1 \leq y_1, Y_2 \leq y_2) &= P(X_1 \leq y_1, X_2 \leq y_2) + P(X_2 \leq y_1, X_1 \leq y_2) \\
		&= 2 P(X_1 \leq y_1) P(X_2 \leq y_2) \\
		&= 2 y_1 y_2 \text{ for } 0 < y_1 < y_2 < 1 \\
		f_{Y_1, Y_2} (y_1, y_2) &= \frac{d}{d{y_2}} \frac{d}{d{y_1}} \\
		&= 2 \text{ for } 0 < y_1 < y_2 < 1 \\
		\mathbb{E}(Y_2 - Y_1) &= \int\limits_0^1 \int\limits_0^{\frac{y}{2}} (y_2 - y_1) 2 dy_1 dy_2 \\
		&= \dots = \frac{1}{3}
	\end{align*}

# Linearity of Expectation
- If $X_1, X_2, \dots X_n$ are jointly distributed then $$ \mathbb{E}\Big( \sum\limits_{i = 1}^n X_i \Big) = \sum\limits_{i = 1}^n \mathbb{E}(X_i)$$
-
\begin{align*}
	\mathbb{E}(X_1 + X_2) &= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x_1 + x_2 f_{X_1, X_2}(x_1, x_2) dx_1 dx_2 \\
	&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x_1 f_{X_1, X_2}(x_1, x_2) dx_1 dx_2 + \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x_2 f_{X_1, X_2}(x_1, x_2) dx_1 dx_2 \\
	&= \int\limits_{-\infty}^{\infty} \int\limits_{-\infty}^{\infty} x_1 f_{X_1, X_2}(x_1, x_2) dx_2 dx_1 + \int\limits_{-\infty}^{\infty}  x_2 \int\limits_{-\infty}^{\infty} f_{X_1, X_2}(x_1, x_2) dx_1 dx_2 \\
	&= \mathbb{E}(X_1) + \mathbb{E}(X_2)
\end{align*}
- Hypergeometric: $N$ trials - $M$ successes, $N - M$ failures.
	- $X_i = 
	\begin{cases}
		1 & \text{ if ith is a success} \\
		0 & \text{ otherwise}
	\end{cases}$
	- $$\sum\limits_{i = 1}^{n} X_i \sim \text{ Hypergeometric}$$
	- 
	\begin{align*}
		\mathbb{E}(\sum\limits_{i = 1}^{n} X_i) &= \sum\limits_{i = 1}^{n} \mathbb{E}(X_i) \\
		&= \sum\limits_{i = 1}^{n} \mathbb{E}(X_1) \\
		&= n \mathbb{E}(X_1) \\
		&= \frac{nM}{N}
	\end{align*}

# Expectation and Independence
- If $X, Y$ are independent and $g, h$ are any real-valued functions, then $$\mathbb{E} \Big( g(X) h(Y) \Big) = \mathbb{E} \Big( g(X)\Big) \mathbb{E}\Big( h(Y)\Big)$$
	- Provided $g, h$ are such that expected values exist!
- Proof: Assume $X, Y$ jointly continuous.
-
\begin{align*}
	\mathbb{E} \Big( g(X) h(Y) \Big) &= \int\limits_0^{\infty} \int\limits_0^{\infty} g(x) h(y) f_{X, Y}(x, y) dx dy \\
	&= \int\limits_0^{\infty} \int\limits_0^{\infty} g(x) h(y) f_X(x) f_Y(y) dx dy \\
	&= \int\limits_0^{\infty} h(y) f_Y(y) \Big( \int\limits_0^{\infty} g(x) f_X(x) dx \Big) dy \\
	&= \int\limits_0^{\infty} h(y) f_Y(y) \Big( \mathbb{E}(g(x)) dy \\
	&= \mathbb{E} \Big( g(X)\Big) \mathbb{E}\Big( h(Y)\Big)
\end{align*}
- In particular, if $X, Y$ are independent and $\mathbb{E}(X), \mathbb{E}(Y)$ exist, then $$\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(XY)$$
	- Says $X, Y$ are *uncorrelated*
		- Uncorrelated means there is no linear function relating them
		- Egregious example: $Z, Z^2$
- Define covariance:
-
\begin{align*}
Cov(X, Y) &= \mathbb{E}\Big[ (X - \mathbb{E}(X)) (Y - \mathbb{E}(Y))\Big] \\
&= \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y)
\end{align*}
	- *Bilinear form*