---
title: Intro Prob Lecture Notes
author:
    - William Sun
date:
    - April 3, 2017
geometry: margin=3cm
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---
# Independence of Random Variables
- If $X_1, X_2, \dots X_n$ are a collection of jointly distributed random variables, then we say that they are (mutually) independent if their joint distribution factors as the product of the individual marginal distributions
	- For example, if $X_1, X_2, \dots X_n$ are jointly discrete then they will be independent if 
		- $$P_{X_1, X_2, \dots X_n}(x_1, x_2, \dots x_n) = P_{X_1}(x_1)P_{X_2}(x_2) \dots P_{X_n}(x_n)$$
		- for *all* possible values of $x_1, x_2, \dots x_n$
	- Also, if $X_1, X_2, \dots X_n$ are jointly continuous then they will be independent if 
		- $$f(x_1, x_2, \dots x_n) = f_{X_1}(x_1)f_{X_2}(x_2) \dots f_{X_n}(x_n)$$
		- for all $x_1, x_2, \dots x_n$
- Example: Dart board
	- $f(x_1, x_2) = \frac{1}{\pi}$ if $x_1^2 + x_2^2 = \leq 1$, 0 otherwise.
	- Compute the marginals
		- If $x < -1$ or $x > 1$, marginal is 0. Otherwise:
		- $$f_{X_1}(x_1) = \int\limits_{-\infty}^{\infty} f(x_1, x_2) d_{x_2} = \int\limits_{-\sqrt{1 - x_1^2}}^{+ \sqrt{1 - x_1^2}} \frac{1}{\pi} d_{x_2} = \frac{2\sqrt{1 - x_1^2}}{\pi}$$
		- Similarly, when in range, $$f_{X_2}(x_2) = \frac{2\sqrt{1 - x_2^2}}{\pi}$$
	- Multiply the marginals
		- $f(\frac{1}{2}, \frac{1}{2}) = \frac{1}{\pi}$
		- $$f_{X_1}(\frac{1}{2}) f_{X_2}(\frac{1}{2}) = \frac{2\sqrt{1 - \frac{1}{4}}}{\pi} \cdot \frac{2\sqrt{1 - \frac{1}{4}}}{\pi} = \frac{3}{\pi^2}$$
		- $\frac{1}{\pi} \neq \frac{3}{\pi^2}$, so $X_1$, $X_2$ are *dependent*.
- Usually independence is an assumption that is made
	- $X_1, \dots X_n$ are independent N($\mu, \sigma^2$)
	- $$\overline{X}_n = \frac{X_1 + \dots + X_n}{n}$$
	- $$S^2 = \frac{\sum\limits_{i = 1}^n(X_i - \overline{X}_n)^2}{n - 1}$$
	- are, in fact, independent (to be proved later)

# Sums of random variables
- If $X_1, X_2$ are jointly distributed random variables, then what is the distribution (pmf) of $X_1 + X_2$?
- Case 1: Suppose $X_1, X_2$ jointly discrete, $P_{X_1, X_2}(x_1, x_2) = P(X_1 = x_1, X_2 = x_2)$ given
	- $P_{X_1, X_2}(u) = P(X_1 + X_2 = u) = \sum\limits_{x_1} P(X_1 + X_2, X_1 = x_1) = \sum\limits_{x_1}P(X_1 = x_1, X_2 = u - x) = \sum\limits_{x_1}P_{X_1, X_2}(x_1, u - x_1)$
		- (Law of total probability)
	- Formula: $$P_{X_1 + X_2}(u) = \sum\limits_{x_1}P_{X_1, X_2}(x_1, u - x_1)$$
	- A common assumption is that $X_1, X_2$ independent. In this case:
		- $$P_{X_1 + X_2}(u) = \sum\limits_{x_1}P_{X_1}(x_1)P_{X_2}(u - x_1)$$
		- Convolution $(P_{X_1} * P_{X_2})(u)$
- Binomial theorem $(a + b)^n = \sum\limits_{k = 0}^{n}\binom{n}{k} a^k b^{n - k}$
	- Example: $X_1 \sim$ Poisson($\lambda_1$), $X_2 \sim$ Poisson($\lambda_2$), and they are independent
		- 
		\begin{align*}
			P_{X_1 + X_2}(u) &= \sum\limits_{x_1 = 0}^{\mathit{\infty}} \frac{e^{-\lambda_1} \lambda_1^{x_1}}{x_1!} \cdot \frac{e^{-\lambda_2}\lambda_2^{u - x_1}}{(u - x_1)!} \\
			&= \frac{e^{-(\lambda_1 + \lambda_2)}}{u!} \sum\limits_{x_1 = 0}^{u} \frac{u!}{x! (u - x_1)!} \cdot \lambda_1^{x_1} \lambda_2^{u - x_1} \\
			&= e^{-(\lambda_1 + \lambda_2)} \frac{(\lambda_1 + \lambda_2)^u}{u!}
		\end{align*}
			- The pdf of a Poisson($\lambda_1 + \lambda_2$)
		 	- Note: For formula to be valid, upper limit must be $\infty$
- Suppose $X_1 + X_2 = n$. $P(X_1 = k \vert X_1 + X_2 = n) = ?$
	-
	\begin{align*}
		P(X_1 = k \vert X_1 + X_2 = n) &= \frac{P(X_1 = k, X_1 + X_2 = n)}{e^{-(\lambda_1 + \lambda_2)} \frac{(\lambda_1 + \lambda_2)^n}{n!}} \\
		&= \frac{e^{-\lambda_1}\frac{\lambda_1^k}{k!} \cdot e^{-\lambda_2} \frac{\lambda_2^{n - k}}{(n - k)!}}{e^{-(\lambda_1 + \lambda_2)} \frac{(\lambda_1 + \lambda_2)^n}{n!}}
	\end{align*}