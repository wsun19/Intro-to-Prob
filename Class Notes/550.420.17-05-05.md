---
title: Intro Prob Lecture Notes
author:
    - William Sun
date:
    - May 5, 2017
geometry: margin=3cm
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---
# The Chebyshev Inequalit(ies)
- For any random variable $X$ with mean $\mu$ and variance $\sigma^2$, we have for any real number $k > 0$, $$P(|X - \mu|) \geq k) \leq \frac{\sigma^2}{k^2}$$
- Equivalently (for the complement) , $$P(|X - \mu| < k) \geq 1 - \frac{\sigma^2}{k^2}$$
- Also equivalent to $$P(| X - \mu| < k \sigma) \geq 1 - \frac{1}{k^2}$$ and $$P(|X - \mu| \geq k \sigma) \leq \frac{1}{k^2}$$
- Remember: Markov's Inequality is a corollary
- It also has a very important application to:

# Weak Law of Large Numbers
- If $X_1, X_2, \dots$ are independentrandom variables each having mean $\mu$ and variance $\sigma^2$, then $$P \Big( \Big| \frac{X_1 + \dots + X_n}{n} - \mu \Big| \geq \epsilon \Big) \rightarrow 0 \text{ as } n \rightarrow \infty$$
	- (Strong Law of Large Numbers): $$P \Big( \lim_{n \rightarrow \infty} \Big| \frac{X_1 + \dots + X_n}{n} - \mu \Big| = 0\Big) = 1$$
		- Too hard for us to prove, so we just prove the weak law
- Proof of Weak Law of Large Numbers:
	- $\overline{X} = \frac{X_1 + \dots + X_n}{n}$.
		- $E(\overline{X}_n) = \mu$
		- $Var(\overline{X}_n) = \frac{\sigma^2}{n}$
	- So apply Chebyshev's inequality
		- $$P(|\overline{X}_n - \mu|) \geq k) \leq \frac{\sigma^2}{nk^2} \rightarrow 0 \text{ as } n \rightarrow \infty$$
- Remark: When (in WLLN) we also have identically distributed, the Central Limit Theorem $\implies$ WLLN.
	- We use the following fact (about the tail behavior of the standard normal distribution): $$(\frac{1}{x} - \frac{1}{x^3}) e^{-\frac{x^2}{2}} < \int\limits_x^{\infty} e^{-\frac{z^2}{2}} dz < \frac{1}{x}e^{-\frac{x^2}{2}}$$ for any x > 0. In other words, when $x \rightarrow \infty$, the ratio between the second two terms is 1.
	- $$P \Big( \Big| \frac{\sum\limits_{i = 1}^n X_i - n \mu}{n} \Big| \geq \epsilon \Big) = P\Big( \Big| \frac{\sum\limits_{i = 1}^n X_i - n \mu}{\sqrt{n}} \Big| \geq \epsilon \sqrt{n} \Big) \approx \frac{2}{\sqrt{2 \pi}} \int\limits_{\epsilon \sqrt{n}}^{\infty} e^{-\frac{z^2}{2}} dz$$
	- As $n \rightarrow \infty$, we approach $$\frac{2}{\sqrt{2 \pi}} \cdot \frac{1}{\epsilon \sqrt{n}} e^{- \frac{\epsilon^2 n}{2}}$$

# Monte Carlo Method
- Ex: 
	- $U \sim$ uniform(0, 1)
	- $h: [0, 1] \rightarrow \mathbb{R}$
	- $X = h(U)$
	- Question: $E(X)$?
		- $E(X) = E(h(U)) = \int\limits_0^1 h(u) du$
		- What if we can't integrate $h$ in closed form?
	- Suppose sequence $U_1, U_2, U_3, \dots \sim$ uniform(0, 1).
		- $X_i = h(U_i)$ $i = 1, 2, \dots$
		- $E(X_i) = \int\limits_0^1 h(u) du$
		- $Var(X_i) = E(X_i^2) - \Big( \int\limits_0^1 h(u) du \Big)^2 = \int\limits_0^1 h^2(u) du - \Big( \int\limits_0^1 h(u) du \Big)^2$
		- $Var(X_i) \leq \int\limits_0^1 h^2(u) du \leq 1$ if $\Big| h(x) \Big| \leq 1$
	- WLLN $\implies$ $P \Big( \Big| \frac{\sum\limits_{i = 1}^n h(U_i)}{n} - \int\limits_0^1 h(u) du \Big| \geq \epsilon \Big) \leq \frac{\sigma^2}{\epsilon^2 n}$
		- Given $\epsilon = .1$, $\frac{1}{(.1)^2 n} \leq .01 \implies n \geq 10^4$
	- Use randomness to solve deterministic problems

# Mixture Distribution (Not on final)
- 2 coins: coin 0 has $\frac{1}{2}$ chance of head, coin 1 has $\frac{1}{3}$ chance of head
- Select mechanism: Pick coin 0 .4, coin 1 .6.
- Flip twice, let $X$ count the number of heads.
- $X_0 \sim binom(2, \frac{1}{2})$, $X_1 \sim binom(2, \frac{1}{3})$
	- $X \sim .4 \binom(2, \frac{1}{2}) + .6 binom(2, \frac{1}{3})$
	- pmf of mixture: $.4 \binom{2}{k} (\frac{1}{2})^2 + .6 \binom{2}{k} (\frac{1}{3})^k (\frac{2}{3})^{2 - k}$
		- $P(X = 2) = P(coin 0) P(X = 2 | coin 0) + P(coin 1)P(X = 2 | coin 1) = .4 (\frac{1}{2})^2 + .6 (\frac{1}{3})^2 = \frac{1}{6}$
	- $X_1 \sim F_1$, $X_2 \sim F_2$, $X_k \sim F_k$ 
		- $F = \sum\limits_j = 1}^k p_j F_j$