---
title: Intro Prob Lecture Notes
author:
    - William Sun
date:
    - May 3, 2017
geometry: margin=3cm
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---

# A few applications of the Central Limit Theorem
- Example: A professor plans to grade 91 exams sequentially. The time sto finish grading a single exam are independent *(continuous)* random variables all having the same distribution with mean $\mu = .25$ hours and standard deviation $\sigma = .1$ hours. 
	- Estimate the probability it takes at least 24 hours to finish grading. $S_{91} = T_1 + T_2 + \dots + T_{91}$ is the total completion time.
	-
	\begin{align*}
		P \Big( S_{91} \geq 24) &= P(\frac{S_{91} = 91(.25)}{.1 \sqrt{91}} \geq \frac{24 - 91(.25)}{.1 \sqrt{91}} \Big) \\
		&\approx 1 - \Phi(1.31) \\
		&\approx .0951
	\end{align*}
	- Now, estimate the probability that at least 20 exams are graded in the first 4 hours. $N_4$ is the number of exams graded by time 4
	- Note: 20 might not be a large enough sample for the Central Limit Theorem to be accurate, but we don't hav eany other approximation methods right now.
	- $$P(N_4 \geq 20) ... ???$$ but we can't recognize $N_4$ as the sum of i.i.d. random variables, so how would we apply the Central Limit?
		- There's a duality principle between $S_n$ and $N_t$: $$(N_t \geq n) = (S_n \leq t)$$
	- 
	\begin{align*}
		P(N_4 \geq 20) = P(S_{20} \leq 4) &= P \Big( \frac{S_{20} - 20(.25)}{.1 \sqrt{20}} \leq \frac{4 - 20(.25)}{.1 \sqrt{20}} \Big) \\
		&= \Phi(-2.24)
	\end{align*}

# Stirling's Approximation for n!
$$n! \approx \sqrt{2 \pi n} \cdot (\frac{n}{e})^n$$
- This means that the two sides converge as $n \rightarrow \infty$
- Ex: 10! = 3,628,800. Stirling Approximation: 3,598,695.61874
- In formulas with factorials, this approximation makes it easier to understand large-number behavior
- Let $X_1, X_2, \dots \sim$ i.i.d. Poisson(1). Let $S_n = \sum\limits_{i = 1}^n X_i$. We saw before that $S_n$ is a ??? (fill in later)
- $P(S_n = n) = \frac{e^{-n} n^n}{n!}$
- On the other hand,
-
\begin{align*}
	P(S_n = n) &= P(\frac{S_n - n}{\sqrt{n}} = \frac{n - n}{\sqrt{n}}) \\
	&\approx 0
\end{align*}
- Not a great approximation. Let's apply a *continuity correction*
- 
\begin{align*}
	P(S_n = n) &= P(n - \frac{1}{2} \leq S_n \leq n + \frac{1}{2}) \\
	&= P(-\frac{1}{2 \sqrt{n}} \leq \frac{S_n - n}{\sqrt{n}} \leq \frac{1}{2\sqrt{n}})\\
	&\approx \int\limits_{-\frac{1}{2 \sqrt{n}}}^{\frac{1}{2 \sqrt{n}}} \frac{e^{-\frac{z^2}{2}}}{\sqrt{2 \pi}} dz \\
	&\approx \frac{1}{\sqrt{2 \pi}} \cdot \frac{1}{\sqrt{n}} \\
	\rightarrow \frac{e^{-n}n^n}{n!} &\approx \frac{1}{\sqrt{2 \pi n}}
\end{align*}
- Interesting application of the Stirling formula: partitions of integer n
	- We know the answer is $\binom{n - 1 + r}{r}$. Fix $r$, let $n \rightarrow \infty$.  $= \frac{(n - 1 + r)!}{r! (n - 1)!} \approx c(r) n^r$. 

# Makrov Inequality
- If $X \geq 0$ random variable, for any constant $a \geq -$ the probability $P(X \geq a) \leq \frac{E(X)}{a}$
- Chebyshev inequality is a corollary
- Important concept, subtracting a positive quantity to produce a lower bound
-
\begin{align*}
	E(X) &= \int\limits_0^{\infty} x f(x) dx \\
	&= \int\limits_0^{a} x f(x) dx + \int\limits_a^{\infty} x f(x) dx \\
	E(X) &\geq \int\limits_a^{\infty} x f(x) dx \geq \int\limits_a^{\infty} a f(x) dx = a P(X \geq a) \\
	\rightarrow P(X \geq a) &\leq \frac{E(X)}{a}
\end{align*}
- Chebyshev Inequality: Now suppose $X$ is any random variable with a finite mean $\mu$ and (not necessarily finite) variance $\sigma^2$
-
\begin{align*}
	P( | X - \mu | \geq k) &= P((X - \mu)^2 \geq k^2) \leq \frac{E((X - \mu)^2)}{k^2} = \frac{\sigma^2}{k^2} \\
	\rightarrow P( | X - \mu | \geq k) &\leq \frac{\sigma^2}{k^2}
\end{align*}

# Weak Law of Large Numbers
