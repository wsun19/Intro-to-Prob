---
title: Intro Prob Lecture Notes
author:
    - William Sun 
date:
    - April 12, 2017
geometry: margin=3cm
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---
# Transformation Method (or method of Jacobians)
- For finding distributions of functions of continuous random variables
- (2-d) Theorem: Suppose $X, Y$ are jointly continuous with joint pdf $f_{X, Y}(x, y) with support $A$ and $u = g_1(x, y)$ and $v = g_2(x, y)$ is a one-to-one transformation of $A$ into $B$. Then the inverse transformation is $$x = h_1(u, v) \text{ and } y = h_2(u, v)$$
- and the joint pdf of $U, V$ is of the form $$f_{U, V}(u, v) = f_{X, Y}(h_1(u, v), h_2(u, v)) \lvert J \rvert$$
- where $J$ is the determinant of 
$\begin{bmatrix}
	\frac{dx}{du} & \frac{dx}{dv} \\
	\frac{dy}{du} & \frac{dy}{dv}
\end{bmatrix}$ = $\frac{dx}{du} \cdot \frac{dy}{dv} - \frac{dx}{dv} \cdot \frac{dy}{du}$ (the Jacobian determinant)
- Why do we care?
	- Ex: $V, R$ independent random variables $f_V(v), f_R(r)$, and $I = \frac{V}{R}$ (electricity)
	- Ex: $X_1, X_2, \dots, X_{100}$ and $\overline{X} = \frac{X_1 + X_2 + \dots + X_{100}}{100}$
		- Find $S^2 = \frac{\sum^n (X_i - \overline{X})^2}{n - 1}$
	- Ex: Suppose $X, Y$ are independent, $X \sim$ Gamma($\alpha, 1$) and $Y \sim$ Gamma($\beta, 1$)
		- $$\rightarrow f_{X, Y}(x, y) = \frac{x^{\alpha - 1} e^{-x} y^{\beta - 1} e^{-y}}{\Gamma(\alpha) \Gamma(\beta)} \text{ for } A = \{x > 0, y > 0\}$$
		- Define $U = \frac{X}{X + Y}$, $V = X + Y$, find the joint pdf of $U, V$
			- $x = uv, y = v(1 - u) = y$
		- $J = det
		\begin{bmatrix}
			v & u \\
			-v & 1 - u
		\end{bmatrix} = v(1 - u) - u(-v) = v$
		- 
		\begin{align*}
			f_{U, V}(u, v) &= f_{X, Y}(uv, v(1 - u)) \cdot \lvert v \rvert \\
			&= \frac{(uv)^{\alpha - 1} e^{-uv} \cdot (v(1 - u))^{\beta - 1} e^{-v(1 - u)} \cdot v}{\Gamma(\alpha) \Gamma(\beta)} \\
			&= \frac{v^{\alpha + \beta - 1} e^{-v} \cdot \Gamma(\alpha + \beta)}{\Gamma(\alpha + \beta)} \cdot \frac{ u^{\alpha - 1} (1 - u)^{\beta - 1}}{\Gamma(\alpha) \Gamma(\beta)}
		\end{align*}
		- Note: check! $$f_U(u) = \frac{\Gamma(\alpha + \beta)u^{\alpha - 1}(1 - u)^{\beta - 1}}{\Gamma(\alpha)\Gamma(\beta)}$$
		- $$f_V(v) = \frac{v^{\alpha + \beta - 1}e^{-v}}{\Gamma(\alpha + \beta)}$$
		- In particular, $U, V$ are independent and $U \sim$ Beta($\alpha, \beta$)
		- Remark: If allwe cared about was finding the distribution of $U = \frac{X}{X + Y}$, then we have a choice for $V$. For instance $U = \frac{X}{X + Y}$ and $V = X$ is one-to-one, as is $V = Y$.
- Quick remark about homework: $f_{X | X + Y}(x | u) = \frac{f_{X, Y}(x, u - x)}{f_{X + Y}(u)}$
	- $f_{X, X + Y}(x, u) = f_{X, Y}(x, u - x)$. Let $x = u, y = u - x, \lvert J \rvert = 1$ and see this was an example of the Transformation Method!
- Ex: $Z \sim \Phi(z)$, $U = \Phi(Z) \sim$ Uniform(0, 1). $\Phi^{-1}(U) = Z$
	- $Z_1, Z_2 \sim$ i.i.d. N(0, 1)$ $$f_{Z1, Z_2}(z_1, z_2) = \frac{e^{-\frac{1}{2}(z_1^2 + z_2^2)}}{2 \pi} (-\infty < z_1, z_2 < \infty)$$
	- Let $R = \sqrt{Z_1^2 + Z_2^2}$, $\Theta = arctan(\frac{Z_2}{Z_1}) \in (0, 2 \pi]$
		- $\rightarrow Z_1 = R cos(\Theta), z_2 = R sin(\Theta), \lvert J \rvert = r$
	-
	\begin{align*}
		f_{R, \Theta}(r, \theta) &= f_{Z_1, Z_2}(r cos(\theta), r sin(\theta)) \cdot r \\
		&= \frac{r e^{-\frac{1}{2}((r cos(\theta))^2 + (r sin(\theta))^2)}}{2 \pi} \\
		&= r e^{-\frac{r^2}{2}} \cdot \frac{1}{2 \pi}
	\end{align*}
	-
	\begin{align*}
		f_{\Theta}(\theta) &= \frac{1}{2 \pi} \text{ for } 0 < \theta \leq 2 \pi \\
		f_{R}(r) &= r e^{-\frac{r^2}{2}} \text{ Rayleigh distribution}
	\end{align*}
		- $F_R(r) = 1 - e^{-\frac{r^2}{2}}$
	- Since $f_{R, \Theta} = f_{R} \cdot f_{\Theta}$, $R, \Theta$ independent
	- Geometry: Uniform($0, 1$) is easy. $2 \pi$ Uniform($0, 1$) = Uniform($0, 2 \pi$)
		- Let $u \sim$ uniform(0, 1)
			- 
			\begin{align*}
			u &= 1 - e^{-\frac{r^2}{2}} \rightarrow r \\
			&= (-2 ln(1 - u))^{\frac{1}{2}} \\
			&= (-2 ln(U_2))^{\frac{1}{2}} cos(2 \pi U_1) \\
			&= (-2 ln(U_2))^{\frac{1}{2}} sin(2 \pi U_1)
			\end{align*}