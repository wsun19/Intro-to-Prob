---
title: Intro Prob Lecture Notes
author:
    - William Sun
date:
    - April 26, 2017
geometry: margin=3cm
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---
# Conditional Expectation
- Law of total expectation $E(X) = E(E(X | Y))$
	- Example: random variable $N$ $p(N = n) = p_n$ for $n = 0, 1, 2, 3, \dots$ $x_1, x_2, x_3, \dots$ i.i.d. and independent of $N$
		- $\rightarrow E(S) = \mu_x \mu_N$
	- $Var(S) = E(S^2) - (E(S))^2 = E(S^2) - \mu_X^2 \mu_N^2$ 
		- $(\sum\limits_{i = 1}^N x_i)(\sum\limits_{j = 1}^N x_j) = \sum\limits_{i = 1}^N \sum\limits_{j = 1}^N x_i x_j = \sum\limits_{i = 1}^N x_i^2 \sum\limits_{i \neq j}^N x_i x_j$
	-
	\begin{align*}
		E(S^2) &= E(E(S^2 | N)) \\
		&= E(\sum\limits_{i = 1}^N x_i^2 \sum\limits_{i \neq j}^N x_i x_j | N = n) \\
		&= E(\sum\limits_{i = 1}^N x_i^2 \sum\limits_{i \neq j}^N x_i x_j) \\
		&= nE(x_1^2) + n(n - 1) E(x_1 x_2) \\
		&= n(\sigma_X^2 + \mu_X^2) + n(n - 1) E(x_1) E(x_2) \\
		&= n \sigma_X^2 + n \mu_X^2 + n(n - 1) \mu_x^2 \\
		&= n \sigma_X^2 + n^2 \mu_X^2
	\end{align*}
	-
	\begin{align*}
		E(S^2) &= E(E(S^2 | N)) \\
		&= E(n \sigma_X^2 + n^2 \mu_X^2) \\
		&= \sum\limits_{n = 0}^{\infty}(n \sigma_X^2 + n^2 \mu_X^2) P_N(n)
		&= \sigma_X^2 \sum\limits_{n = 0}^{\infty} n P_N(n) + \mu_X^2 \sum\limits_{n = 0}^{\infty} n^2 P_N(n) \\
		&= \sigma_X^2 \mu_N + \sigma_X^2 E(N^2) \\
	\end{align*}
	- $$Var(S) = \sigma_X^2 \mu_N + \mu_X^2 E(N^2) - \mu_X^2 \mu_N^2 = \sigma^2 \mu_N + \mu_X^2 \sigma_N^2$$

## Function with another variable
- Let $X, Y$ r.v.'s and $h$ is any function. Then $$E(X h(Y) | Y) = h(Y) E(X | Y)$$ or $$E(X h(Y) | Y = y) = h(y) E(X | Y = y)$$
- Proof:
-
\begin{align*}
	E(Xh(Y) | Y = y) &= \int\limits_{-\infty}^{\infty} x h(y) f_{X | Y}(x, y) dx \\
	&= h(y) \int\limits_{-\infty}^{\infty} x f_{X | Y}(x | y) dx \\
	&= h(y) E(X | Y = y)
\end{align*}
- 
\begin{align*}
	E(g(X)h(Y)) &= E(E(g(X)h(Y)  Y))\\
	&= E(h(Y) E(g(X) | Y))
\end{align*}
- Ex: $X \sim$ geometric($p$)
	- Let $Y$ be the outcome on the first toss. $f_Y(y) = \begin{cases}
		p & y = 1 \\
		1 - p & y = 0
	\end{cases}$
	- $E(X) = E(X | Y = 0) + E(X | Y = 1) = (1 + E(X))(1 - p) + 1(p) = 1 \cdot p + (1 - p)(1 + \mu) = \frac{1}{p}$
	- $Var(X) = E(X^2) - (E(X))^2$
		- 
		\begin{align*}
			E(X^2) &= E(E(X^2 | Y)) \\
			&= E(X^2 Y = 1)p + E(X^2 | Y = 0)(1 - p) \\
			&= 1 \cdot p + E((1 + X)^2)(1 - p) \\
			&= p + E(1 + 2X + X^2)(1 - p) \\
			&= p + (1 - p) + (1 - p)\frac{2}{p} + E(X^2)(! - p) \\
			pE(X^2) &= 1 + \frac{2(1 - p)}{p} \\
			E(X^2) &= \frac{1}{p} + \frac{2(1 - p)}{p^2}
		\end{align*}
		- So $$Var(X) = \frac{1}{p} + \frac{2(1 - p)}{p^2} - (\frac{1}{p})^2 = \frac{1 - p}{p^2}$$

# Conditional Variance
- $$Var(X | Y = y) = E([X - E(X | Y = y)] ^2 | Y = y)$$
- $$f(x, y) = xe^{-x(1 + y)} \text{ for } x > 0, y > 0$$
- $$f_Y(y) = \frac{1}{(1 + y)^2} for y > 0$$
- $$f_{X | Y}(x | y) = (1 + y)^2 xe^{-x(1 + y)} \text{ for } x > 0$$
- Gamma(2, $\frac{1}{1 + y}$)
- $X | Y = y \sim$ Gamma($\alpha, \beta$) with $\alpha = 2, \beta = \frac{1}{1 + y}$

# Moment Generating Functions