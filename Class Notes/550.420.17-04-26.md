---
title: Intro Prob Lecture Notes
author:
    - William Sun
date:
    - April 26, 2017
geometry: margin=3cm
header-includes:
    - \usepackage{setspace}
    - \doublespacing
---
# Conditional Expectation
- Law of total expectation $E(X) = E(E(X | Y))$
	- Example: random variable $N$ $p(N = n) = p_n$ for $n = 0, 1, 2, 3, \dots$ $x_1, x_2, x_3, \dots$ i.i.d. and independent of $N$
		- $\rightarrow E(S) = \mu_x \mu_N$
	- $Var(S) = E(S^2) - (E(S))^2 = E(S^2) - \mu_X^2 \mu_N^2$ 
		- $(\sum\limits_{i = 1}^N x_i)(\sum\limits_{j = 1}^N x_j) = \sum\limits_{i = 1}^N \sum\limits_{j = 1}^N x_i x_j = \sum\limits_{i = 1}^N x_i^2 \sum\limits_{i \neq j}^N x_i x_j$
	-
	\begin{align*}
		E(S^2) &= E(E(S^2 | N)) \\
		&= E(\sum\limits_{i = 1}^N x_i^2 \sum\limits_{i \neq j}^N x_i x_j | N = n) \\
		&= E(\sum\limits_{i = 1}^N x_i^2 \sum\limits_{i \neq j}^N x_i x_j) \\
		&= nE(x_1^2) + n(n - 1) E(x_1 x_2) \\
		&= n(\sigma_X^2 + \mu_X^2) + n(n - 1) E(x_1) E(x_2) \\
		&= n \sigma_X^2 + n \mu_X^2 + n(n - 1) \mu_x^2 \\
		&= n \sigma_X^2 + n^2 \mu_X^2
	\end{align*}
	-
	\begin{align*}
		E(S^2) &= E(E(S^2 | N)) \\
		&= E(n \sigma_X^2 + n^2 \mu_X^2) \\
		&= \sum\limits_{n = 0}^{\infty}(n \sigma_X^2 + n^2 \mu_X^2) P_N(n)
		&= \sigma_X^2 \sum\limits_{n = 0}^{\infty} n P_N(n) + \mu_X^2 \sum\limits_{n = 0}^{\infty} n^2 P_N(n) \\
		&= \sigma_X^2 \mu_N + \sigma_X^2 E(N^2) \\
	\end{align*}
	- $$Var(S) = \sigma_X^2 \mu_N + \mu_X^2 E(N^2) - \mu_X^2 \mu_N^2 = \sigma^2 \mu_N + \mu_X^2 \sigma_N^2$$

## Function with another variable
- Let $X, Y$ r.v.'s and $h$ is any function. Then $$E(X h(Y) | Y) = h(Y) E(X | Y)$$ or $$E(X h(Y) | Y = y) = h(y) E(X | Y = y)$$
- Proof:
-
\begin{align*}
	E(Xh(Y) | Y = y) &= \int\limits_{-\infty}^{\infty} x h(y) f_{X | Y}(x, y) dx \\
	&= h(y) \int\limits_{-\infty}^{\infty} x f_{X | Y}(x | y) dx \\
	&= h(y) E(X | Y = y)
\end{align*}
- 
\begin{align*}
	E(g(X)h(Y)) &= E(E(g(X)h(Y)  Y))\\
	&= E(h(Y) E(g(X) | Y))
\end{align*}
- Ex: $X \sim$ geometric($p$)
	- Let $Y$ be the outcome on the first toss. $f_Y(y) = \begin{cases}
		p & y = 1 \\
		1 - p & y = 0
	\end{cases}$
	- $E(X) = E(X | Y = 0) + E(X | Y = 1) = (1 + E(X))(1 - p) + 1(p) = 1 \cdot p + (1 - p)(1 + \mu) = \frac{1}{p}$
	- $Var(X) = E(X^2) - (E(X))^2$
		- 
		\begin{align*}
			E(X^2) &= E(E(X^2 | Y)) \\
			&= E(X^2 Y = 1)p + E(X^2 | Y = 0)(1 - p) \\
			&= 1 \cdot p + E((1 + X)^2)(1 - p) \\
			&= p + E(1 + 2X + X^2)(1 - p) \\
			&= p + (1 - p) + (1 - p)\frac{2}{p} + E(X^2)(! - p) \\
			pE(X^2) &= 1 + \frac{2(1 - p)}{p} \\
			E(X^2) &= \frac{1}{p} + \frac{2(1 - p)}{p^2}
		\end{align*}
		- So $$Var(X) = \frac{1}{p} + \frac{2(1 - p)}{p^2} - (\frac{1}{p})^2 = \frac{1 - p}{p^2}$$

# Conditional Variance
- $$Var(X | Y = y) = E([X - E(X | Y = y)] ^2 | Y = y)$$
- $$f(x, y) = xe^{-x(1 + y)} \text{ for } x > 0, y > 0$$
- $$f_Y(y) = \frac{1}{(1 + y)^2} for y > 0$$
- $$f_{X | Y}(x | y) = (1 + y)^2 xe^{-x(1 + y)} \text{ for } x > 0$$
- Gamma(2, $\frac{1}{1 + y}$)
- $X | Y = y \sim$ Gamma($\alpha, \beta$) with $\alpha = 2, \beta = \frac{1}{1 + y}$
	- Means $= \alpha \beta = \frac{2}{1 + y}$
	- Variance $= \alpha \beta^2 = \frac{2}{(1 + y)^2}$
-
\begin{align*}
	Var(X | Y = y) &= E([X - E(X | Y = y)]^2 | Y = y) \\
	&= E( \Big[ X - \frac{2}{1 + y} \Big]^2 | Y = y) \\
	&= E(X^2 - \frac{4}{1 + y}X + \frac{4}{(1 + y)^2} | Y = y) \\
	&= \int\limits_0^{\infty} x^2 f_{X | Y}(x, y) dx - \frac{4}{1 + y} \int\limits_0^{\infty} x f_{X | Y}(x, y) dx + \frac{4}{(1 + y)^2} \\
	&= (1 + y^2) \int\limits_0^{\infty} x^2 xe^{-x(1 + y)} dx - \frac{4}{1 + y} \frac{2}{1 + y} + \frac{4}{( 1 + y)^2} \\
	&= \frac{(1 + y)^2 \cdot 3!}{(1 + y)^4} - \frac{8}{(1 + y)^2} + \frac{4}{(1 + y)^2} \\
	&= \frac{2}{(1 + y)^2}
\end{align*}
- Example: $Z_1, Z_2$ independent standard normals.
	- $X = \mu_X + \sigma_X Z_1$
	- $Y = \mu_Y + \sigma_Y \rho Z_1 + \sigma_{X(?)} \sqrt{1 - \rho^2} Z_2$
	- $Z_1 = \frac{X - \mu_X}{\sigma_X}$
	- $Y = \mu_Y + \sigma_Y \rho \Big( \frac{X - \mu_X}{\sigma_X} \Big) + \sigma_Y \sqrt{1 - \rho^2} Z_2$
	- $Y | X \sim$ Normal($\mu_Y + \frac{\sigma_Y}{\sigma_X} \rho (x - \mu_X), \sigma_Y^2 (1 - \rho^2))$
	- Then:
	- $E(Y | X) = \mu_Y + \frac{\sigma_Y \rho(X - \mu_X)}{\sigma_Y^2 (1 - \rho^2)}$
	- $Var(Y | X) = \sigma_Y^2 (1 - \rho^2)$
	- $E(Y | X) = \mu_Y - \frac{\sigma_Y \mu_X \rho}{\sigma_X} + \frac{\sigma_{Y(?)} \rho}{\sigma_X} X$
	- $Var(E(Y | X)) = \frac{\sigma_Y^2 \rho^2}{\sigma_X^2} Var(X) = \sigma_Y^2 \rho^2$
		- $\implies \rho^2 = \frac{Var(E(Y | X))}{Var(Y)}$

# Moment Generating Functions
- For a random variable X, $M_X(s) = E(e^{sX})$ (when this function is finite/exists in an open neighborhood of s = 0)
-
\begin{align*}
	e^y &= 1 + y + \frac{y^2}{2!} + \frac{y^3}{3!} + \dots \\
	e^{sX} &= 1 + sX + \frac{sX^2}{2!} + \frac{sX^3}{3!} + \dots \\
	E(e^{sX}) &= 1 + E(X)s + E(X^2)\frac{s^2}{2!} + \dots \\
	&= \sum\limits_{n = 0}{\infty} E(X^n) \frac{s^n}{n!} = M_X(s)
\end{align*}
- Suppose $X \sim$ uniform(0, 1). Compute $M_X(s)$
-
\begin{align*}
	M_X(s) &= E(e^{sX}) \\
	&= \int\limits_0^1 e^{sX} 1 dx \\
	&= \frac{e^{sX}}{s} \Big|_{x = 0}^1 \\
	&= \frac{e^s - 1}{s} \\
\end{align*}
-
\begin{align*}
	e^s &= 1 + s + \frac{s^2}{2!} + \frac{s^3}{3!} + \dots \\
	e^s - 1 &= s + \frac{s^2}{2!} + \frac{s^3}{3!} + \dots \\
	\frac{e^s - 1}{s} &= 1 + \frac{s}{2!} + \frac{s^2}{3!} + \dots + \frac{s^{n - 1}}{n!} + \frac{s^{n (?)}}{(n + 1)!} \\
	&= 1 + (\frac{1}{2})s + (\frac{1}{3}) \frac{s^2}{2} + \dots + (\frac{1}{n})\frac{s^{n - 1}}{(n - 1)!} + (\frac{1}{n + 1})\frac{s^n}{n!} + \dots \\
	E(X^n) &= \frac{1}{n + 1}
\end{align*}
- Application of mgf:
	- $M'_X(s) = \frac{d}{ds} (M_X(s)) = \frac{d}{ds} E(e^{sX}) = E(\frac{d}{ds}(e^{sX})) = E(Xe^{sX})$
	- $\mathbf{M'_X(0) = E(X)}$
	- $\frac{d^n}{ds^n} M'_X(s) = E(X^n e^{sX})$
	- $\mathbf{\frac{d^n}{ds^n}(M_X(0)) = E(X^n)}$